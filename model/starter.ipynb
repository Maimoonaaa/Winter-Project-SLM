{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae1d8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer norm\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim):\n",
    "        super().__init__()\n",
    "        self.weight= nn.Parameter(torch.ones(ndim)) #weight is a vector, not a matrix- elemnent vise scaling happens when Wx+b happens- if W were a matrix, it eats the purpose of norm and it would just become a linear layer.\n",
    "        self.bias=nn.Parameter(torch.zeros(ndim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        return F.layer_norm(x,self.weight.shape,self.weight, self.bias,1e-5)\n",
    "    \n",
    "\n",
    "\n",
    "#causal/masked self attention head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.head_size=config.n_embed // config.n_head\n",
    "        self.k=nn.Linear(config.n_embed, config.n_embed // config.n_head, bias=config.bias)\n",
    "        self.q=nn.Linear(config.n_embed, config.n_embed // config.n_head, bias=config.bias)\n",
    "        self.v=nn.Linear(config.n_embed, config.n_embed // config.n_head, bias=config.bias)\n",
    "        self.register_buffer(\"tril\",torch.tril(torch.ones(config.block_size,config.block_size)))\n",
    "        self.dropout=nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C= x.size()\n",
    "        #head_size= n_embed/n_head\n",
    "        q=self.q(x)\n",
    "        k=self.k(x)\n",
    "        v=self.v(x) #BxTxhead_size\n",
    "        wei= q @ k.transpose(-2,-1)* (1.0 / math.sqrt(k.size(-1))) #BxTxT\n",
    "        wei=wei.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei=self.dropout(wei)\n",
    "        out= wei @ v  #BxTxT @ BxTxhead_size= BxTxhead_size\n",
    "        return out\n",
    "\n",
    "class MaskedMultiAtt(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.heads= nn.ModuleList([Head(config) for _ in range(config.n_head)])\n",
    "        self.proj= nn.Linear(config.n_embed,config.n_embed)\n",
    "        self.dropout=nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out= torch.cat([h(x) for h in self.heads], dim=-1) #concatenates the all the n_head heads of size n_embed/n_heads --> output is BxTxC\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out \n",
    "\n",
    "\n",
    "\n",
    "#Multi-Layer-Perceptron\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc=nn.Linear(config.n_embed,4* config.n_embed)   #probably should change this 4x to 3x or smaller\n",
    "        self.gelu= nn.GELU()\n",
    "        self.proj= nn.Linear(4* config.n_head, config.n_embed)\n",
    "        self.dropout=nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.c_fc(x)\n",
    "        x=self.gelu(x)\n",
    "        x=self.proj(x)\n",
    "        x=self.dropout(x)\n",
    "        return x\n",
    "\n",
    "#Block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1=LayerNorm(config.n_embed,config.bias)\n",
    "        self.attn=MaskedMultiAtt(config)\n",
    "        self.ln2=LayerNorm(config.n_embed,config.bias)\n",
    "        self.mlp= MLP(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=x+self.attn(self.ln1(x))\n",
    "        x=x+self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "#defining size of model and structure\n",
    "@dataclass\n",
    "class SLMconfig:\n",
    "    n_embed:int =384\n",
    "    vocab_size:int =65\n",
    "    block_size:int =64\n",
    "    n_layer:int =4\n",
    "    n_head:int =6\n",
    "    bias: bool= False\n",
    "    dropout: float= 0.0\n",
    "\n",
    "\n",
    "#final architecture\n",
    "class SLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config=config\n",
    "\n",
    "        self.transformer= nn.ModuleDict(dict(\n",
    "            wt_tok_em=nn.Embedding(config.vocab_size,config.n_embed),\n",
    "            wt_pos_emb=nn.Embedding(config.block_size,config.n_embed),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_head)\n",
    "\n",
    "        ))\n",
    "        self.ln_head=nn.Linear(config.n_embed,config.n_vocab,bias=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
