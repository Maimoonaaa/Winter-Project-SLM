{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae1d8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c248a6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Layer norm\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim):\n",
    "        super().__init__()\n",
    "        self.weight= nn.Parameter(torch.ones(ndim)) #weight is a vector, not a matrix- elemnent vise scaling happens when Wx+b happens- if W were a matrix, it eats the purpose of norm and it would just become a linear layer.\n",
    "        self.bias=nn.Parameter(torch.zeros(ndim))\n",
    "\n",
    "    def forward(self,x):\n",
    "        return F.layer_norm(x,self.weight.shape,self.weight, self.bias,1e-5)\n",
    "    \n",
    "\n",
    "\n",
    "#causal/masked self attention head\n",
    "class Head(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.head_size=config.n_embed // config.n_head\n",
    "        self.k=nn.Linear(config.n_embed, config.n_embed // config.n_head, bias=config.bias)\n",
    "        self.q=nn.Linear(config.n_embed, config.n_embed // config.n_head, bias=config.bias)\n",
    "        self.v=nn.Linear(config.n_embed, config.n_embed // config.n_head, bias=config.bias)\n",
    "        self.register_buffer(\"tril\",torch.tril(torch.ones(config.block_size,config.block_size)))\n",
    "        self.dropout=nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C= x.size()\n",
    "        #head_size= n_embed/n_head\n",
    "        q=self.q(x)\n",
    "        k=self.k(x)\n",
    "        v=self.v(x) #BxTxhead_size\n",
    "        wei= q @ k.transpose(-2,-1)* (1.0 / math.sqrt(k.size(-1))) #BxTxT\n",
    "        wei=wei.masked_fill(self.tril[:T,:T]==0,float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei=self.dropout(wei)\n",
    "        out= wei @ v  #BxTxT @ BxTxhead_size= BxTxhead_size\n",
    "        return out\n",
    "\n",
    "class MaskedMultiAtt(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.heads= nn.ModuleList([Head(config) for _ in range(config.n_head)])\n",
    "        self.proj= nn.Linear(config.n_embed,config.n_embed)\n",
    "        self.dropout=nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out= torch.cat([h(x) for h in self.heads], dim=-1) #concatenates the all the n_head heads of size n_embed/n_heads --> output is BxTxC\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out \n",
    "\n",
    "\n",
    "\n",
    "#Multi-Layer-Perceptron\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc=nn.Linear(config.n_embed,4* config.n_embed)   #probably should change this 4x to 3x or smaller\n",
    "        self.gelu= nn.GELU()\n",
    "        self.proj= nn.Linear(4* config.n_head, config.n_embed)\n",
    "        self.dropout=nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.c_fc(x)\n",
    "        x=self.gelu(x)\n",
    "        x=self.proj(x)\n",
    "        x=self.dropout(x)\n",
    "        return x\n",
    "\n",
    "#Block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1=LayerNorm(config.n_embed,config.bias)\n",
    "        self.attn=MaskedMultiAtt(config)\n",
    "        self.ln2=LayerNorm(config.n_embed,config.bias)\n",
    "        self.mlp= MLP(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=x+self.attn(self.ln1(x))\n",
    "        x=x+self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "#defining size of model and structure\n",
    "@dataclass\n",
    "class SLMconfig:\n",
    "    n_embed:int =384\n",
    "    vocab_size:int =65\n",
    "    block_size:int =64\n",
    "    n_layer:int =4\n",
    "    n_head:int =6\n",
    "    bias: bool= False\n",
    "    dropout: float= 0.0\n",
    "\n",
    "\n",
    "#final architecture\n",
    "class SLM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config=config\n",
    "\n",
    "        self.transformer= nn.ModuleDict(dict(\n",
    "            wt_tok_em=nn.Embedding(config.vocab_size,config.n_embed),\n",
    "            wt_pos_emb=nn.Embedding(config.block_size,config.n_embed),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_head)\n",
    "\n",
    "        ))\n",
    "        self.lm_head=nn.Linear(config.n_embed,config.n_vocab,bias=False)\n",
    "        self.transformer.wt_tok_em.weight= self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "        \n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wt_pos_emb.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self,config):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "        tok_em=self.transformer.wt_tok_em(idx) # b x t x n_embed\n",
    "        pos_em=self.transformer.wt_tok_em(pos) # bxtxn_embed\n",
    "        x=self.transformer.drop(tok_em+pos_em)\n",
    "        for block in self.transformer.h:\n",
    "            x=block(x)\n",
    "        x=self.transformer.ln_f(x)\n",
    "        if targets is not None:\n",
    "            logits=self.lm_head(x) #b x t x vocab_size\n",
    "            loss=F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1),ignore_index=-1)\n",
    "        else:\n",
    "            d#uring inference no loss is calc\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def crop_blocksize(self, block_size):\n",
    "        assert block_size <= self.config.block_size\n",
    "         self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        #verify\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx,max_new_tokens, temperature=1.0, top_k=None):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond= idx if idx.size(-1)<= self.config.block_size else idx[:,-self.config.block_size,:]\n",
    "            logits, _= self(idx_cond)\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs= F.softmax(logits, dim=-1)\n",
    "            idx_next= torch.multinomial(probs, nu_samples=1,)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38df87f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
